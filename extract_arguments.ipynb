{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import Google's trained word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format ('models/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare the data by encoding every sentence as a sequence of word2vec-encoded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_abo = pd.read_csv(\"data/abortion.tsv\", sep = \"\\t\")\n",
    "raw_data_clo = pd.read_csv(\"data/cloning.tsv\", sep = \"\\t\")\n",
    "raw_data_dp = pd.read_csv(\"data/death_penalty.tsv\", sep = \"\\t\") \n",
    "raw_data_gun = pd.read_csv(\"data/gun_control.tsv\", sep = \"\\t\")\n",
    "raw_data_mari = pd.read_csv(\"data/marijuana_legalization.tsv\", sep = \"\\t\")\n",
    "raw_data_wage = pd.read_csv(\"data/minimum_wage.tsv\", sep = \"\\t\")\n",
    "raw_data_nuc = pd.read_csv(\"data/nuclear_energy.tsv\", sep = \"\\t\")\n",
    "raw_data_school = pd.read_csv(\"data/school_uniforms.tsv\", sep = \"\\t\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [raw_data_abo, raw_data_clo, raw_data_dp, raw_data_gun, raw_data_mari, raw_data_wage, raw_data_nuc, raw_data_school]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat(frames, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 24507 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The combined dataset contains {0} rows and {1} columns\".format(len(raw_data), len(raw_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to keep three columns: The topic to compute the topic-relevance of a sentence, the sentence itself and the label of the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[[\"topic\", \"sentence\", \"annotation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>sentence</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abortion</td>\n",
       "      <td>This means it has to steer monetary policy to ...</td>\n",
       "      <td>NoArgument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abortion</td>\n",
       "      <td>Where did you get that ?</td>\n",
       "      <td>NoArgument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abortion</td>\n",
       "      <td>Nathanson later became pro-life .</td>\n",
       "      <td>NoArgument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abortion</td>\n",
       "      <td>In this case we may never do evil ( directly a...</td>\n",
       "      <td>Argument_against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abortion</td>\n",
       "      <td>With that I would like to give everyone someth...</td>\n",
       "      <td>NoArgument</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                           sentence  \\\n",
       "0  abortion  This means it has to steer monetary policy to ...   \n",
       "1  abortion                           Where did you get that ?   \n",
       "2  abortion                  Nathanson later became pro-life .   \n",
       "3  abortion  In this case we may never do evil ( directly a...   \n",
       "4  abortion  With that I would like to give everyone someth...   \n",
       "\n",
       "         annotation  \n",
       "0        NoArgument  \n",
       "1        NoArgument  \n",
       "2        NoArgument  \n",
       "3  Argument_against  \n",
       "4        NoArgument  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def encode_sentences(data):\n",
    "    N_sentences = len(data) \n",
    "    encoded_sentences = []\n",
    "    print(\"---------------------Now encoding sentences!---------------------\")\n",
    "    print(\"Max iterations:\", N_sentences)\n",
    "    # shuffle the dataframe rows\n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    labels = data.annotation.copy()\n",
    "    labels[labels == \"NoArgument\"] = 0\n",
    "    labels[labels != 0] = 1\n",
    "    labels = labels.values\n",
    "    \n",
    "    \n",
    "    # take the topic that the sentence comes from,\n",
    "    # to compute topic relevance\n",
    "    topics = data.topic\n",
    "    topics = list(topics)\n",
    "    \n",
    "    # Store the different amount of word counts,\n",
    "    # together with the indices of the sentences\n",
    "    # that contain this amount of words\n",
    "    word_counts = {}\n",
    "    \n",
    "    max_words = 0\n",
    "    \n",
    "    \n",
    "    # for each sentence:\n",
    "    for i in range(N_sentences):\n",
    "        # take the sentence from the dataframe\n",
    "        sentence = data.sentence.iloc[i]\n",
    "        # tokenize the sentence\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        # filter puncuation and stop words from the tokens\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if(token[0] not in \".,:;[](){}!?-_`'~\\\"^/1234567890\"):\n",
    "                words.append(token)\n",
    "        N_words = len(words)\n",
    "        \n",
    "        # keep track of the maximum sentence length\n",
    "        if(N_words > max_words):\n",
    "            max_words = N_words\n",
    "        \n",
    "        \n",
    "        # if this amount of words has been\n",
    "        # encountered before, add the index\n",
    "        # of the sentence\n",
    "        if(N_words in word_counts):\n",
    "            word_counts[N_words].append(i)\n",
    "        # else, create new entry with index\n",
    "        else:\n",
    "             word_counts[N_words] = [i]\n",
    "     \n",
    "        # encode topic and add similarity of sentence to topic\n",
    "        # as additional feature\n",
    "        topic = topics[i]\n",
    "        topic_words = topic.split()\n",
    "        topic_vectors = []\n",
    "        # compute the average word vector for the topic\n",
    "        for word in topic_words:\n",
    "            if(word in word2vec):\n",
    "                word_vector = word2vec[word]\n",
    "            else:\n",
    "                word_vector = np.random.uniform(low = -0.01, high = 0.01, size = (300))\n",
    "            topic_vectors.append(word_vector)\n",
    "        topic_vectors = np.asarray(topic_vectors)\n",
    "        avg_topic_vector = np.mean(topic_vectors, axis = 0)\n",
    "            \n",
    "        # store a sentence as a sequence of word vectors\n",
    "        sequence = []\n",
    "        for word in words:\n",
    "            # embed a word using the Google word2vec model,\n",
    "            # if it exists in the dictionary\n",
    "            if(word in word2vec):\n",
    "                 word_vector = word2vec[word]\n",
    "            # if word does not exist in the word2vec model, \n",
    "            # add a randomized word vector instead\n",
    "            else:\n",
    "                word_vector = np.random.uniform(low = -0.01, high = 0.01, size = (300))\n",
    "        \n",
    "            \n",
    "            # compute similarity between word and topic, then add as feature\n",
    "            similarity = cosine_similarity([word_vector], [avg_topic_vector])\n",
    "            #print(\"Current word:\", word, \"Curren topic:\",  topic,  \"similarity:\", similarity)\n",
    "            word_vector = np.append(word_vector, similarity)     \n",
    "            # add word to the sequence\n",
    "            sequence.append(word_vector)\n",
    "        # convert list sequence to numpy array for convenience\n",
    "        sequence = np.asarray(sequence)\n",
    "        # print progress every 1000 epochs\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"iteration :\", i )\n",
    "        encoded_sentences.append(sequence)\n",
    "        \n",
    "    encoded_sentences = np.asarray(encoded_sentences)\n",
    "    \n",
    "    \n",
    "    \"\"\"print(\"Now zero padding..\")\n",
    "    for i in range(N_sentences):\n",
    "        # print progress every 1000 epochs\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"iteration :\", i )\n",
    "        # compute how much zero padding is needed\n",
    "        N_words = len(encoded_sentences[i])\n",
    "        padding_needed = max_words - N_words\n",
    "        for j in range(padding_needed):\n",
    "            encoded_sentences[i] = np.append(encoded_sentences[i], [None], axis = 0)\"\"\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    # create batches to speed-up training\n",
    "    # group sentences with equal word counts into the same batches\n",
    "    all_batches = []\n",
    "    label_batches = []\n",
    "    #print(max_words)\n",
    "    #print(word_counts)\n",
    "    for count in word_counts:\n",
    "        # get the sentences with this amount of words\n",
    "        sentence_idx = word_counts[count]\n",
    "        batch = []\n",
    "        label_batch = []\n",
    "        # add each sentence with this amount of words\n",
    "        # to the batch\n",
    "        for idx in sentence_idx:\n",
    "            batch.append(encoded_sentences[idx])\n",
    "            label_batch.append(labels[idx])\n",
    "            #print(label_batch)\n",
    "        batch = np.asarray(batch)\n",
    "        label_batch = np.asarray(label_batch)\n",
    "        \n",
    "        all_batches.append(batch)\n",
    "        label_batches.append(label_batch)\n",
    "        \n",
    "    all_batches = np.asarray(all_batches)\n",
    "    label_batches = np.asarray(label_batches)\n",
    "    # now, all the different batches are stored in an\n",
    "    # array, where each batch can be accessed by an \n",
    "    # index\n",
    "    return all_batches, label_batches, data\n",
    "    \n",
    "    \n",
    "    #return encoded_sentences, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the annotation as the labels and convert all the arguments to the positive class and the non-arguments to the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Now encoding sentences!---------------------\n",
      "Max iterations: 24507\n",
      "iteration : 0\n",
      "iteration : 1000\n",
      "iteration : 2000\n",
      "iteration : 3000\n",
      "iteration : 4000\n",
      "iteration : 5000\n",
      "iteration : 6000\n",
      "iteration : 7000\n",
      "iteration : 8000\n",
      "iteration : 9000\n",
      "iteration : 10000\n",
      "iteration : 11000\n",
      "iteration : 12000\n",
      "iteration : 13000\n",
      "iteration : 14000\n",
      "iteration : 15000\n",
      "iteration : 16000\n",
      "iteration : 17000\n",
      "iteration : 18000\n",
      "iteration : 19000\n",
      "iteration : 20000\n",
      "iteration : 21000\n",
      "iteration : 22000\n",
      "iteration : 23000\n",
      "iteration : 24000\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences, labels, shuffled_data = encode_sentences(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_sentences, labels = encode_sentences(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_sentences[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encoded_sentences.shape)\n",
    "#encoded_sentences = pad_sequences(encoded_sentences, padding = \"post\", value = np.zeros(301), maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(encoded_sentences)\n",
    "train_test_split = 0.5\n",
    "validation_size = (1 - train_test_split) / 2\n",
    "x_train = encoded_sentences[:int(train_test_split*N)]\n",
    "y_train = labels[:int(train_test_split*N)]\n",
    "\n",
    "x_val = encoded_sentences[int(train_test_split*N) : int(train_test_split*N) + int(validation_size*N)]\n",
    "y_val = labels[int(train_test_split*N) : int(train_test_split*N) + int(validation_size*N)]\n",
    "\n",
    "x_test = encoded_sentences[int(train_test_split*N) + int(validation_size*N):]\n",
    "y_test = labels[int(train_test_split*N) + int(validation_size*N):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the Keras LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "def init_model(size = 50, dropout = 0.5, learning_rate = 0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(size), merge_mode='concat', input_shape=(None, 301)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, labels):\n",
    "    total_accuracy = 0\n",
    "    for i in range(len(data)):\n",
    "        score = model.evaluate(data[i], labels[i], verbose = 0)\n",
    "        #print(score)\n",
    "        total_accuracy += score[1]\n",
    "    total_accuracy = total_accuracy / len(data)\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold(data, labels, folds, epochs):\n",
    "    kf = KFold(n_splits=folds)\n",
    "\n",
    "    avg_accuracy = 0\n",
    "    current_fold = 0\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # reset the classifier\n",
    "        model = init_model()\n",
    "        \n",
    "        \n",
    "        Xtrain = data[train_index]\n",
    "        Xtest = data[test_index]\n",
    "        \n",
    "        Ytrain = labels[train_index]\n",
    "        Ytest = labels[test_index]\n",
    "        \n",
    "        # train on the train data\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(Xtrain)):\n",
    "                model.fit(Xtrain[i], Ytrain[i], epochs=1, verbose = 0, batch_size = Xtrain[i].shape[0])\n",
    "            # get train accuracy\n",
    "            train_acc = get_accuracy(Xtrain, Ytrain)\n",
    "            print(\"Fold: {0} \\n Epoch: {1} \\n Train accuracy: {2}\".format(current_fold, e, train_acc))\n",
    "                \n",
    "        # test on the test data\n",
    "        test_acc = get_accuracy(Xtest, Ytest)\n",
    "        print(\"Fold: {0} \\n Test accuracy: {2}\".format(current_fold, test_acc))\n",
    "        avg_accuracy += test_acc \n",
    "        current_fold += 1\n",
    "            \n",
    "    avg_accuracy /= folds\n",
    "    print(\"{0}-fold cross validation accuracy: {1}\".format(folds, avg_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 \n",
      " Epoch: 0 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 1 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 2 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 3 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 4 \n",
      " Train accuracy: 0.5035470218752873\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ef8887babf5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-6f85e9c35fdf>\u001b[0m in \u001b[0;36mkfold\u001b[1;34m(data, labels, folds, epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m# get train accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold(encoded_sentences, labels, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of batches: 81\n",
      "--------------Training epoch:-------------- 0\n",
      "Train accuracy: 0.5392116505289613\n",
      "Validation accuracy: 0.7178196883746898\n",
      "Test accuracy: 0.6504065045496312\n",
      "--------------Training epoch:-------------- 1\n",
      "Train accuracy: 0.6005991931810143\n",
      "Validation accuracy: 0.7391974508755617\n",
      "Test accuracy: 0.5967479682550197\n",
      "--------------Training epoch:-------------- 2\n",
      "Train accuracy: 0.6709745601016542\n",
      "Validation accuracy: 0.753552104379902\n",
      "Test accuracy: 0.7601626021106068\n",
      "--------------Training epoch:-------------- 3\n",
      "Train accuracy: 0.67814133639153\n",
      "Validation accuracy: 0.7186955129106585\n",
      "Test accuracy: 0.7626016270823595\n",
      "--------------Training epoch:-------------- 4\n",
      "Train accuracy: 0.710803036037304\n",
      "Validation accuracy: 0.7141176266880583\n",
      "Test accuracy: 0.7235772362569484\n",
      "--------------Training epoch:-------------- 5\n",
      "Train accuracy: 0.7276038380919336\n",
      "Validation accuracy: 0.724523761518034\n",
      "Test accuracy: 0.7195121958488371\n",
      "--------------Training epoch:-------------- 6\n",
      "Train accuracy: 0.7539211649392751\n",
      "Validation accuracy: 0.7449175960592601\n",
      "Test accuracy: 0.6788617895870674\n",
      "--------------Training epoch:-------------- 7\n",
      "Train accuracy: 0.769018387385529\n",
      "Validation accuracy: 0.7316250877104596\n",
      "Test accuracy: 0.7195121958488371\n",
      "--------------Training epoch:-------------- 8\n",
      "Train accuracy: 0.7941608379725136\n",
      "Validation accuracy: 0.7196269793265617\n",
      "Test accuracy: 0.7520325212943845\n",
      "--------------Training epoch:-------------- 9\n",
      "Train accuracy: 0.8019738038327252\n",
      "Validation accuracy: 0.7229694725217701\n",
      "Test accuracy: 0.7520325212943845\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of batches:\", len(x_train))\n",
    "for e in range(epochs):\n",
    "    print(\"--------------Training epoch:--------------\", e)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # shuffle training data and labels\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(x_train)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(y_train)\n",
    "    np.random.seed()\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        correct = 0\n",
    "        total += len(x_train[i])\n",
    "        history = model.fit(x_train[i], y_train[i], epochs=1, verbose = 0, batch_size = x_train[i].shape[0])\n",
    "        acc = history.history['acc'][0]\n",
    "        #print(\"Batch-accuracy:\", acc, \"Samples:\", len(encoded_sentences[i]))\n",
    "\n",
    "        correct = acc * len(x_train[i])\n",
    "        total_correct += correct\n",
    "    print(\"Train accuracy:\", total_correct / total)\n",
    "    print(\"Validation accuracy:\", get_accuracy(x_val, y_val))\n",
    "    print(\"Test accuracy:\", get_accuracy(x_test, y_test))\n",
    "    \n",
    "    #print(\"Accuracy:\", get_accuracy(encoded_sentences, labels))   \n",
    "        \n",
    "\n",
    "    #acc = get_accuracy(x_train, y_train)   \n",
    "    #print(\"Train accuracy:\", acc)\n",
    "    #acc = get_accuracy(x_test, y_test)   \n",
    "    #print(\"Test accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7333333337306976\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\", get_accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"models/bilstm_cos.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "#model = keras.models.load_model(\"models/bilstm_cos.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\", get_accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_accuracy() missing 2 required positional arguments: 'data' and 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-904c5ff9b04a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_accuracy() missing 2 required positional arguments: 'data' and 'labels'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
