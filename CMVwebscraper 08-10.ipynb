{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "url = \"https://old.reddit.com/r/changemyview/\"\n",
    "\n",
    "\n",
    "##two vectors of topics and urls\n",
    "topics, urls = linksAndTopics(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMV: Climate change should be the the focus in upcoming elections.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://old.reddit.com/r/changemyview/comments/9mro8r/cmv_climate_change_should_be_the_the_focus_in/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(topics[0])\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "txt, OPwords, OPname = getOPtext(urls[0])\n",
    "\n",
    "#print(newURL + \"\\n\")\n",
    "#print(topics[0]+ \"\\n\")\n",
    "#print(txt + \"\\n\")\n",
    "#print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getComments(url):\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            #link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "            \n",
    "    return(extracted_comments, textComm, names)\n",
    "\n",
    "extracted_comments, textComm, names = getComments(urls[0])\n",
    "print(len(extracted_comments))\n",
    "\n",
    "print(textComm[0].split())\n",
    "##get individual commments and split it into words\n",
    "def bagOfWords(textComm):\n",
    "    words = []\n",
    "    for i in range(1,len(textComm)):\n",
    "        words.append(textComm[i].split)\n",
    "    return(words)\n",
    "\n",
    "words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all functions together:\n",
    "## soup = souper(url)\n",
    "## topics, urls = linksAndTopics(url)\n",
    "## txt, OPwords, name = getOPtext(url)\n",
    "## extracted_comments, textComm, names = getComments(url)\n",
    "## words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "CMV: People who support the #MeToo movement should protest Bill Clinton’s arena tour due to his history of being accused of sexual assault.\n",
      "https://old.reddit.com/r/changemyview/comments/9mjmri/cmv_people_who_support_the_metoo_movement_should/\n",
      "Final Edit: Thanks all for the great discussion. It was thought provoking and I appreciated the time that people put into their arguments. My feelings on this issue didn’t change, but I saw the flaws in my argument and I’m going to think about how I can better present my case. I simply want all powerful men to be held to an equally high standard. No passes for being on the right team.I got a lot of hope that this generation of left voters are holding their representatives to higher standards and that women are making progress. It sometimes doesn’t feel that way, especially with Kavanaugh and the heartbreak that brought on, but where we were in the 90’s vs now...wow. Fun fact: Women were not allowed to wear pants on the U.S. Senate floor until 1993. —————I saw the news that Bill and Hillary Clinton will be touring together, and on the heels of seeing all of my friends protesting Kavanaugh and grieving over what this means for the #metoo movement, I find any support or excitement for Bill Clinton to be extremely problematic. If people believed Brett Kavanaugh’s accuser, (Christine Blasey Ford), they should also believe Bill Clinton’s accusers (Juanita Broaddrick, Kathleen Willey, Paula Jones, Leslie Millwee). All of these women have been accused of being politically motivated in their accusations, but the left believes Bill Clinton and gives him a pass because he’s on our side. I find this to be extremely hypocritical. We should be protesting his events (“An Evening With The Clintons”), and not allowing another accused sexual abuser to have a platform for shaping this country. Edit to add link detailing accusations against Clinton. I kinda forgot that a lot of people aren’t aware of the severity of the accusations: Four women over the past few decades have publicly accused Bill Clinton of sexual assault or harassment. One woman accused Clinton of raping her. \n",
      "/u/Soreynotsari (OP) has awarded 1 delta(s) in this post.\n",
      "All comments that earned deltas (from OP or other users) are listed here, in /r/DeltaLog.\n",
      "Please note that a change of view doesn't necessarily mean a reversal, or that the conversation has ended.\n",
      "Delta System Explained | Deltaboards\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "def getComments(url):\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            #link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "            \n",
    "    return(extracted_comments, textComm, names)\n",
    "\n",
    "\n",
    "def search(query):\n",
    "    query = query.lower()\n",
    "    TOPIC = None\n",
    "    stop = False\n",
    "    locURL = None\n",
    "    url = \"https://old.reddit.com/r/changemyview/\"\n",
    "    ##two vectors of topics and urls\n",
    "    topics, urls = linksAndTopics(url)\n",
    "    for i in range(len(topics)):\n",
    "        splitTOP = topics[i].split()\n",
    "        for x in range(len(splitTOP)):\n",
    "            word = splitTOP[x].lower()\n",
    "            if word == query:\n",
    "                TOPIC = topics[i]\n",
    "                print(i)\n",
    "                locURL = i\n",
    "                stop = True\n",
    "                break\n",
    "    if stop == False:\n",
    "        return(None,None,None,None,None, \"No topic found\")\n",
    "    else:\n",
    "        ##return all data\n",
    "        URL = urls[locURL]\n",
    "        OPtxt, OPwords, Opname = getOPtext(URL)\n",
    "        comments, commentTXT, Names = getComments(URL)\n",
    "        return(OPname, OPtxt, commentTXT, Names, URL, TOPIC)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "OPname, OPtxt, CommentText, commentName, URL, TOPIC = search(\"#metoo\")\n",
    "\n",
    "print(TOPIC)\n",
    "print(URL)\n",
    "print(OPtxt)\n",
    "print(CommentText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
