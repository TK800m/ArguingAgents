{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "url = \"https://old.reddit.com/r/changemyview/\"\n",
    "\n",
    "\n",
    "##two vectors of topics and urls\n",
    "topics, urls = linksAndTopics(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMV: Dragonflys are significantly doper than spiders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(topics[0])\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "txt, OPwords, OPname = getOPtext(urls[0])\n",
    "\n",
    "#print(newURL + \"\\n\")\n",
    "#print(topics[0]+ \"\\n\")\n",
    "#print(txt + \"\\n\")\n",
    "#print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "['/u/mostlikelynotarobot', '(OP)', 'has', 'awarded', '6', 'delta(s)', 'in', 'this', 'post.', 'All', 'comments', 'that', 'earned', 'deltas', '(from', 'OP', 'or', 'other', 'users)', 'are', 'listed', 'here,', 'in', '/r/DeltaLog.', 'Please', 'note', 'that', 'a', 'change', 'of', 'view', \"doesn't\", 'necessarily', 'mean', 'a', 'reversal,', 'or', 'that', 'the', 'conversation', 'has', 'ended.', 'Delta', 'System', 'Explained', '|', 'Deltaboards']\n"
     ]
    }
   ],
   "source": [
    "def getComments(url):\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            #link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "            \n",
    "    return(extracted_comments, textComm, names)\n",
    "\n",
    "extracted_comments, textComm, names = getComments(urls[0])\n",
    "print(len(extracted_comments))\n",
    "\n",
    "print(textComm[0].split())\n",
    "##get individual commments and split it into words\n",
    "def bagOfWords(textComm):\n",
    "    words = []\n",
    "    for i in range(1,len(textComm)):\n",
    "        words.append(textComm[i].split)\n",
    "    return(words)\n",
    "\n",
    "words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all functions together:\n",
    "## soup = souper(url)\n",
    "## topics, urls = linksAndTopics(url)\n",
    "## txt, OPwords, name = getOPtext(url)\n",
    "## extracted_comments, textComm, names = getComments(url)\n",
    "## words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeltaBot'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMV: It is unreasonable to expect consumers to wait for international releases of media, and piracy is basically justified in these instances.\n",
      "https://old.reddit.com/r/changemyview/comments/9om3j8/cmv_it_is_unreasonable_to_expect_consumers_to/\n",
      "So for example if a TV show releases in America first, but then it takes months for that show to be released in another country, then you shouldn't be surprised if tons of people resort to pirating your show instead. We live in an age of instant gratification and this is just how the world works now, if you aren't prepared to give audiences worldwide a way to pay for and access your content legally and conveniently then that's your problem.I think services like Netflix exemplify this problem - you are offered different content depending on where you are. In that case you can't even use the 'lost sale' argument (which is already a tenuous claim) because you're not paying the creators of the show directly, you're paying Netflix, and if you're already paying a subscription then it makes no difference to the show's creators whether you're watching a particular show today or six months down the line.This is a footnote from the CMV moderators. We'd like to remind you of a couple of things. Firstly, please read through our rules. If you see a comment that has broken one, it is more effective to report it than downvote it. Speaking of which, downvotes don't change views! Any questions or concerns? Feel free to message us. Happy CMVing!\n",
      "/u/silenthunt (OP) has awarded 1 delta(s) in this post.\n",
      "All comments that earned deltas (from OP or other users) are listed here, in /r/DeltaLog.\n",
      "Please note that a change of view doesn't necessarily mean a reversal, or that the conversation has ended.\n",
      "Delta System Explained | Deltaboards\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "def getComments(url):\n",
    "    deltabot = 0\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    delta_link = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "        if commenter == \"DeltaBot\":\n",
    "            deltabot = 1\n",
    "            delta_link.append(link)\n",
    "    return(extracted_comments, textComm, names, delta_link, deltabot)\n",
    "\n",
    "\n",
    "def search(query):\n",
    "    query = query.lower()\n",
    "    TOPIC = None\n",
    "    stop = False\n",
    "    locURL = None\n",
    "    deltabot = None\n",
    "    url = \"https://old.reddit.com/r/changemyview/\"\n",
    "    ##two vectors of topics and urls\n",
    "    topics, urls = linksAndTopics(url)\n",
    "    for i in range(len(topics)):\n",
    "        splitTOP = topics[i].split()\n",
    "        for x in range(len(splitTOP)):\n",
    "            word = splitTOP[x].lower()\n",
    "            if word == query:\n",
    "                TOPIC = topics[i]\n",
    "                #print(i)\n",
    "                locURL = i\n",
    "                stop = True\n",
    "                break\n",
    "    if stop == False:\n",
    "        return(None,None,None,None,None, \"No topic found\")\n",
    "    else:\n",
    "        ##return all data\n",
    "        URL = urls[locURL]\n",
    "        OPtxt, OPwords, Opname = getOPtext(URL)\n",
    "        comments, commentTXT, Names, deltalink, deltabot = getComments(URL)\n",
    "        return(OPname, OPtxt, commentTXT, Names, URL, TOPIC, deltalink, deltabot)   \n",
    "    \n",
    "\n",
    "OPname, OPtxt, CommentText, commentName, URL, TOPIC, deltalink, deltabot= search(\"unreasonable\")\n",
    "\n",
    "print(TOPIC)\n",
    "print(URL)\n",
    "print(OPtxt)\n",
    "print(CommentText[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://old.reddit.com/r/changemyview/comments/9om3j8/cmv_it_is_unreasonable_to_expect_consumers_to/e7v3q2u/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9om3j8/cmv_it_is_unreasonable_to_expect_consumers_to/e7v3q3l/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9om3j8/cmv_it_is_unreasonable_to_expect_consumers_to/e7v4dk1/']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/u/silenthunt', '(OP)', 'has', 'awarded', '1', 'delta(s)', 'in', 'this', 'post.', 'All', 'comments', 'that', 'earned', 'deltas', '(from', 'OP', 'or', 'other', 'users)', 'are', 'listed', 'here,', 'in', '/r/DeltaLog.', 'Please', 'note', 'that', 'a', 'change', 'of', 'view', \"doesn't\", 'necessarily', 'mean', 'a', 'reversal,', 'or', 'that', 'the', 'conversation', 'has', 'ended.', 'Delta', 'System', 'Explained', '|', 'Deltaboards']\n",
      "1\n",
      "https://old.reddit.com/r/changemyview/comments/9om3j8/cmv_it_is_unreasonable_to_expect_consumers_to/e7v3q2u/\n",
      "<a href=\"/r/DeltaLog/comments/9omsby/deltas_awarded_in_cmv_it_is_unreasonable_to/\" rel=\"nofollow\">here</a>\n",
      "https://old.reddit.com/r/DeltaLog/comments/9omsby/deltas_awarded_in_cmv_it_is_unreasonable_to/\n"
     ]
    }
   ],
   "source": [
    "if deltabot == 1:\n",
    "    deltaTXT = CommentText[0]\n",
    "    deltaW = deltaTXT.split()\n",
    "    deltaNumber = int(deltaTXT.split()[4])\n",
    "    print(deltaW)\n",
    "    print(deltaNumber)\n",
    "    print(deltalink[0])\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = deltalink[0]\n",
    "store = []\n",
    "\n",
    "comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            delta = comment.find('div', {'class': 'md'})\n",
    "            if delta.find('a', {'href': True}, text='here')!= None:\n",
    "                store.append(delta.find('a', {'href': True}, text='here'))\n",
    "\n",
    "deltaurl = str(store[0])\n",
    "print(deltaurl)\n",
    "deltaurl = \"https://old.reddit.com\"+ deltaurl[9:-25]\n",
    "\n",
    "print(deltaurl)\n",
    "#print(store)\n",
    "\n",
    "store = []\n",
    "main_table = souper(deltaurl).find(\"div\",attrs={'id':'siteTable'})\n",
    "entries = main_table.find_all(\"div\",attrs={'id':'entry unvoted'})\n",
    "for entry in entries:\n",
    "    if entry.find('form'):\n",
    "        delta = entry.find('div', {'class': 'md'})\n",
    "        all = delta.find_all('a', {'href': True})\n",
    "        \n",
    "deltanames = []\n",
    "for i in range(4,4+(deltaNumber*2),2):\n",
    "    deltanames.append((all[i].text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/u/TomorrowsBreakfast']\n"
     ]
    }
   ],
   "source": [
    "print(deltanames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
