{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "url = \"https://old.reddit.com/r/changemyview/\"\n",
    "\n",
    "\n",
    "##two vectors of topics and urls\n",
    "topics, urls = linksAndTopics(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMV: Dragonflys are significantly doper than spiders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(topics[0])\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "txt, OPwords, OPname = getOPtext(urls[0])\n",
    "\n",
    "#print(newURL + \"\\n\")\n",
    "#print(topics[0]+ \"\\n\")\n",
    "#print(txt + \"\\n\")\n",
    "#print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getComments(url):\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            #link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "            \n",
    "    return(extracted_comments, textComm, names)\n",
    "\n",
    "extracted_comments, textComm, names = getComments(urls[0])\n",
    "print(len(extracted_comments))\n",
    "\n",
    "print(textComm[0].split())\n",
    "##get individual commments and split it into words\n",
    "def bagOfWords(textComm):\n",
    "    words = []\n",
    "    for i in range(1,len(textComm)):\n",
    "        words.append(textComm[i].split)\n",
    "    return(words)\n",
    "\n",
    "words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all functions together:\n",
    "## soup = souper(url)\n",
    "## topics, urls = linksAndTopics(url)\n",
    "## txt, OPwords, name = getOPtext(url)\n",
    "## extracted_comments, textComm, names = getComments(url)\n",
    "## words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMV: Dragonflys are significantly doper than spiders\n",
      "https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/\n",
      "I know reddit is fairly enamored with spiders (/r/spiderbro, etc.), but I think dragonflys are doper.I define dopeness in this context as the combination of net benefit to humans, coolness of abilities, and ability to fly beauty.The last two categories are obviously subjective, but arguable through examples and stuff.Feel free to argue for a different definition of dope.I believe dragonflys are doper than spiders (under this definition) because they:\n",
      "1. Eat pests just like spiders, but without killing humans\n",
      "2. Fly around like a fuckin snitch from Harry Potter, have 4 wings, are super fast, and can predict the flight path of other bugs to intercept them midair.\n",
      "3. Look really fucking cool.\n",
      "/u/mostlikelynotarobot (OP) has awarded 6 delta(s) in this post.\n",
      "All comments that earned deltas (from OP or other users) are listed here, in /r/DeltaLog.\n",
      "Please note that a change of view doesn't necessarily mean a reversal, or that the conversation has ended.\n",
      "Delta System Explained | Deltaboards\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "def getComments(url):\n",
    "    deltabot = 0\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    delta_link = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "        if commenter == \"DeltaBot\":\n",
    "            deltabot = 1\n",
    "            delta_link.append(link)\n",
    "    return(extracted_comments, textComm, names, delta_link)\n",
    "\n",
    "\n",
    "def search(query):\n",
    "    query = query.lower()\n",
    "    TOPIC = None\n",
    "    stop = False\n",
    "    locURL = None\n",
    "    deltabot = None\n",
    "    url = \"https://old.reddit.com/r/changemyview/\"\n",
    "    ##two vectors of topics and urls\n",
    "    topics, urls = linksAndTopics(url)\n",
    "    for i in range(len(topics)):\n",
    "        splitTOP = topics[i].split()\n",
    "        for x in range(len(splitTOP)):\n",
    "            word = splitTOP[x].lower()\n",
    "            if word == query:\n",
    "                TOPIC = topics[i]\n",
    "                #print(i)\n",
    "                locURL = i\n",
    "                stop = True\n",
    "                break\n",
    "    if stop == False:\n",
    "        return(None,None,None,None,None, \"No topic found\")\n",
    "    else:\n",
    "        ##return all data\n",
    "        URL = urls[locURL]\n",
    "        OPtxt, OPwords, Opname = getOPtext(URL)\n",
    "        comments, commentTXT, Names, deltabot = getComments(URL)\n",
    "        return(OPname, OPtxt, commentTXT, Names, URL, TOPIC, deltabot)   \n",
    "    \n",
    "\n",
    "OPname, OPtxt, CommentText, commentName, URL, TOPIC, deltabot= search(\"dragonflys\")\n",
    "\n",
    "print(TOPIC)\n",
    "print(URL)\n",
    "print(OPtxt)\n",
    "print(CommentText[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tdh3z/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tdh3z/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tuid1/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7ttdwu/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tdh6l/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tumoe/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tumoe/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7tv137/',\n",
       " 'https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/e7ty451/']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltabot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "deltabot = [1]\n",
    "print(deltabot[0])\n",
    "if deltabot == 1:\n",
    "    deltaTXT = CommentText[0]\n",
    "    deltaW = deltaTXT.split()\n",
    "    deltaWords = int(deltaTXT.split()[4])\n",
    "    print(deltaW)\n",
    "    print(deltaWords)\n",
    "\n",
    " \n",
    "url = \"https://old.reddit.com/r/changemyview/comments/9ody9p/cmv_dragonflys_are_significantly_doper_than/\"\n",
    "def getDelta(url):\n",
    "    deltabot = 0\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    deltalink = comments.find('a', attrs={'href': re.compile(\"^http://\")})\n",
    "    print(deltalink)\n",
    "    \n",
    "#delta = getDelta(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
