{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "url = \"https://old.reddit.com/r/changemyview/\"\n",
    "\n",
    "\n",
    "##two vectors of topics and urls\n",
    "topics, urls = linksAndTopics(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMV: Climate change should be the the focus in upcoming elections.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://old.reddit.com/r/changemyview/comments/9mro8r/cmv_climate_change_should_be_the_the_focus_in/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(topics[0])\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "txt, OPwords, OPname = getOPtext(urls[0])\n",
    "\n",
    "print(newURL + \"\\n\")\n",
    "print(topics[0]+ \"\\n\")\n",
    "print(txt + \"\\n\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "['/u/7sidedcube', '(OP)', 'has', 'awarded', '2', 'delta(s)', 'in', 'this', 'post.', 'All', 'comments', 'that', 'earned', 'deltas', '(from', 'OP', 'or', 'other', 'users)', 'are', 'listed', 'here,', 'in', '/r/DeltaLog.', 'Please', 'note', 'that', 'a', 'change', 'of', 'view', \"doesn't\", 'necessarily', 'mean', 'a', 'reversal,', 'or', 'that', 'the', 'conversation', 'has', 'ended.', 'Delta', 'System', 'Explained', '|', 'Deltaboards']\n"
     ]
    }
   ],
   "source": [
    "def getComments(url):\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            #link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "            \n",
    "    return(extracted_comments, textComm, names)\n",
    "\n",
    "extracted_comments, textComm, names = getComments(urls[0])\n",
    "print(len(extracted_comments))\n",
    "\n",
    "print(textComm[0].split())\n",
    "##get individual commments and split it into words\n",
    "def bagOfWords(textComm):\n",
    "    words = []\n",
    "    for i in range(1,len(textComm)):\n",
    "        words.append(textComm[i].split)\n",
    "    return(words)\n",
    "\n",
    "words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all functions together:\n",
    "## soup = souper(url)\n",
    "## topics, urls = linksAndTopics(url)\n",
    "## txt, OPwords, name = getOPtext(url)\n",
    "## extracted_comments, textComm, names = getComments(url)\n",
    "## words = bagOfWords(textComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeltaBot'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-e572d466235c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOPname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommentTXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTOPIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mOPname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCommentText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommentName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTOPIC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bill\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#return(OPname, OPtxt, CommentText, CommentName, URL, TOPIC)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-e572d466235c>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinksAndTopics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msplitTOP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "def search(query):\n",
    "    query = query.lower()\n",
    "    TOPIC = None\n",
    "    stop = False\n",
    "    locURL = None\n",
    "    url = \"https://old.reddit.com/r/changemyview/\"\n",
    "    ##two vectors of topics and urls\n",
    "    topics, urls = linksAndTopics(url)\n",
    "    for i in range(len(topics)):\n",
    "        splitTOP = topics[i].split()\n",
    "        for x in range(len(splitTOP)):\n",
    "            word = splitTOP[x].lower()\n",
    "            if word == query:\n",
    "                TOPIC = topics[i]\n",
    "                print(i)\n",
    "                locURL = i\n",
    "                stop = True\n",
    "                break\n",
    "    if stop == False:\n",
    "        return(None,None,None,None,None, \"No topic found\")\n",
    "    else:\n",
    "        ##return all data\n",
    "        URL = urls[locURL]\n",
    "        OPtxt, OPwords, Opname = getOPtext(URL)\n",
    "        comments, commentTXT, Names = getComments(URL)\n",
    "        return(OPname, OPtxt, commentTXT, Names, URL, TOPIC)\n",
    "\n",
    "OPname, OPtxt, CommentText, commentName, URL, TOPIC = search(\"Bill\")\n",
    "\n",
    "#return(OPname, OPtxt, CommentText, CommentName, URL, TOPIC)\n",
    "print(TOPIC)\n",
    "print(URL)\n",
    "print(OPtxt)\n",
    "print(CommentText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def souper(url):\n",
    "    user_agent = 'Thijmen Kupers Student reseaching arguments on CMV: t.kupers@student.rug.nl'\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def linksAndTopics(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    comment_a_tags = main_table.find_all('a',attrs={'class':'bylink comments may-blank'})\n",
    "    #Blank list to store the urls as they are extracted\n",
    "    urls = [] \n",
    "    for a_tag in comment_a_tags:\n",
    "        url = a_tag['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = \"https://reddit.com\"+url\n",
    "        urls.append(url)\n",
    "\n",
    "    titles = main_table.find_all('a',attrs={'class':'title'})\n",
    "    topics= []\n",
    "    for topic in titles:\n",
    "        t = topic.contents[0]\n",
    "        topics.append(t)\n",
    "    return(topics, urls)\n",
    "\n",
    "def getOPtext(url):\n",
    "    main_table = souper(url).find(\"div\",attrs={'id':'siteTable'})\n",
    "    OPtext = main_table.find(\"div\", attrs={'class':'usertext-body'}).findAll('p')\n",
    "    txt = \"\"\n",
    "    for p in range(len(OPtext)):\n",
    "        txt += OPtext[p].text\n",
    "\n",
    "    OPwords = txt.split()\n",
    "\n",
    "\n",
    "    OPname = main_table.find(\"a\", attrs={'class':\"author\"})\n",
    "    name = OPname.text\n",
    "    \n",
    "    return(txt, OPwords, name)\n",
    "\n",
    "def getComments(url):\n",
    "    comment_area = souper(url).find('div',attrs={'class':'commentarea'})\n",
    "    comments = comment_area.find_all('div', attrs={'class':'entry unvoted'})\n",
    "    extracted_comments = []\n",
    "    textComm = []\n",
    "    names = []\n",
    "    for comment in comments: \n",
    "        if comment.find('form'):\n",
    "            commenter = comment.find('a',attrs={'class':'author'}).text\n",
    "            comment_text = comment.find('div',attrs={'class':'md'}).text\n",
    "            #link = comment.find('a',attrs={'class':'bylink'})['href']\n",
    "            extracted_comments.append({'name':commenter,'text':comment_text})\n",
    "            textComm.append(comment_text)\n",
    "            names.append(commenter)\n",
    "            \n",
    "    return(extracted_comments, textComm, names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
