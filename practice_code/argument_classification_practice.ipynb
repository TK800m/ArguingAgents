{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the super important and useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dp = pd.read_csv('data/dp-slider-means.csv')\n",
    "raw_data_evo = pd.read_csv('data/evo-slider-means.csv')\n",
    "raw_data_gc = pd.read_csv('data/gc-slider-means.csv')\n",
    "raw_data_gm = pd.read_csv('data/gm-slider-means.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the amount of rows and columns for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 8 columns\n",
      "Amount of rows:\n",
      " total: 5375 \n",
      " dp: 987 \n",
      " evo: 1252\n",
      " gc: 1590\n",
      " gm: 1546\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(raw_data_dp) + len(raw_data_evo) + len(raw_data_gc) + len(raw_data_gm)\n",
    "\n",
    "print(\"The data contains {0} columns\".format(len(raw_data_dp.columns)))\n",
    "print(\"Amount of rows:\\n total: {0} \\n dp: {1} \\n evo: {2}\\n gc: {3}\\n gm: {4}\".format(n_rows, len(raw_data_dp),\n",
    "                                                                                       len(raw_data_evo), \n",
    "                                                                         len(raw_data_gc), len(raw_data_gm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all four datasets into one data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [raw_data_dp, raw_data_evo, raw_data_gc, raw_data_gm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat(frames, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 5375 rows and 8 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The combined dataset contains {0} rows and {1} columns\".format(len(raw_data), len(raw_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 5 rows of the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>GoodSliderMean</th>\n",
       "      <th>GoodSliderDev</th>\n",
       "      <th>Connective.x</th>\n",
       "      <th>PairType.x</th>\n",
       "      <th>ResponseInitial.x</th>\n",
       "      <th>Phrase.x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>658</td>\n",
       "      <td>ab5810d83f23243ddce713ac23d775cd</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>so</td>\n",
       "      <td>P1_P2</td>\n",
       "      <td>false</td>\n",
       "      <td>Sorry for the length of the post, but I hope i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>871</td>\n",
       "      <td>e0a35a65ce12b2457e8ff1f9b8cec749</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>no_connective</td>\n",
       "      <td>P1_P2</td>\n",
       "      <td>true</td>\n",
       "      <td>I am all for the death penalty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>931</td>\n",
       "      <td>f16863ac9454707946061848c7e9a3e5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>no_connective</td>\n",
       "      <td>P1_P2</td>\n",
       "      <td>true</td>\n",
       "      <td>I am pro death penalty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>936</td>\n",
       "      <td>f1f99c6b1f3f14025a3c01cb8a13b10b</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_connective</td>\n",
       "      <td>P1_P2</td>\n",
       "      <td>false</td>\n",
       "      <td>I can't believe that you just said \"So what if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>029bc4e01ac943f87837556b32d5627a</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>so</td>\n",
       "      <td>QR</td>\n",
       "      <td>false</td>\n",
       "      <td>So what does he have to do with a debate like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                            ItemId  GoodSliderMean  \\\n",
       "0         658  ab5810d83f23243ddce713ac23d775cd           1.000   \n",
       "1         871  e0a35a65ce12b2457e8ff1f9b8cec749           1.000   \n",
       "2         931  f16863ac9454707946061848c7e9a3e5           1.000   \n",
       "3         936  f1f99c6b1f3f14025a3c01cb8a13b10b           1.000   \n",
       "4          11  029bc4e01ac943f87837556b32d5627a           0.999   \n",
       "\n",
       "   GoodSliderDev   Connective.x PairType.x ResponseInitial.x  \\\n",
       "0            NaN             so      P1_P2             false   \n",
       "1       0.000000  no_connective      P1_P2              true   \n",
       "2       0.000000  no_connective      P1_P2              true   \n",
       "3            NaN  no_connective      P1_P2             false   \n",
       "4       0.001414             so         QR             false   \n",
       "\n",
       "                                            Phrase.x  \n",
       "0  Sorry for the length of the post, but I hope i...  \n",
       "1                    I am all for the death penalty.  \n",
       "2                            I am pro death penalty.  \n",
       "3  I can't believe that you just said \"So what if...  \n",
       "4  So what does he have to do with a debate like ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the argument score and the argument itself, so only keep that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[[\"GoodSliderMean\", \"Connective.x\", \"ResponseInitial.x\", \"Phrase.x\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only have the arguments and its annotated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GoodSliderMean</th>\n",
       "      <th>Connective.x</th>\n",
       "      <th>ResponseInitial.x</th>\n",
       "      <th>Phrase.x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>so</td>\n",
       "      <td>false</td>\n",
       "      <td>Sorry for the length of the post, but I hope i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>no_connective</td>\n",
       "      <td>true</td>\n",
       "      <td>I am all for the death penalty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000</td>\n",
       "      <td>no_connective</td>\n",
       "      <td>true</td>\n",
       "      <td>I am pro death penalty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>no_connective</td>\n",
       "      <td>false</td>\n",
       "      <td>I can't believe that you just said \"So what if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999</td>\n",
       "      <td>so</td>\n",
       "      <td>false</td>\n",
       "      <td>So what does he have to do with a debate like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GoodSliderMean   Connective.x ResponseInitial.x  \\\n",
       "0           1.000             so             false   \n",
       "1           1.000  no_connective              true   \n",
       "2           1.000  no_connective              true   \n",
       "3           1.000  no_connective             false   \n",
       "4           0.999             so             false   \n",
       "\n",
       "                                            Phrase.x  \n",
       "0  Sorry for the length of the post, but I hope i...  \n",
       "1                    I am all for the death penalty.  \n",
       "2                            I am pro death penalty.  \n",
       "3  I can't believe that you just said \"So what if...  \n",
       "4  So what does he have to do with a debate like ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ordered by topic and by argument score, so shuffle it before using it for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now properly shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045              but\n",
       "3       no_connective\n",
       "645                so\n",
       "1459               so\n",
       "751                so\n",
       "547                so\n",
       "528             first\n",
       "527                so\n",
       "83              first\n",
       "150                if\n",
       "289                so\n",
       "77                 so\n",
       "33      no_connective\n",
       "891                if\n",
       "79                 if\n",
       "237               but\n",
       "602     no_connective\n",
       "109                so\n",
       "242                if\n",
       "185               but\n",
       "419                so\n",
       "920                if\n",
       "569     no_connective\n",
       "1237              but\n",
       "1088               if\n",
       "281                if\n",
       "1429              but\n",
       "786               but\n",
       "942             first\n",
       "650               but\n",
       "            ...      \n",
       "554               but\n",
       "32                 so\n",
       "408             first\n",
       "191     no_connective\n",
       "291                so\n",
       "299               but\n",
       "589     no_connective\n",
       "384             first\n",
       "816     no_connective\n",
       "1421              but\n",
       "753                so\n",
       "808             first\n",
       "126                so\n",
       "624                if\n",
       "427             first\n",
       "721     no_connective\n",
       "467               but\n",
       "1018    no_connective\n",
       "114             first\n",
       "1013            first\n",
       "84                 if\n",
       "736                if\n",
       "344                so\n",
       "449     no_connective\n",
       "229                so\n",
       "983                so\n",
       "932     no_connective\n",
       "696                if\n",
       "543                if\n",
       "686                if\n",
       "Name: Connective.x, Length: 5375, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"Connective.x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = raw_data[\"Phrase.x\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a threshold for a good argument\n",
    "threshold = 0.5\n",
    "\n",
    "labels = raw_data[\"GoodSliderMean\"].values\n",
    "#negatives = labels[labels < threshold]\n",
    "#positives = labels[labels >= threshold]\n",
    "#print(\"Negatives: {0}, Positives: {1}\".format(len(negatives) / len(raw_data), len(positives) / len(raw_data)))\n",
    "\n",
    "labels[labels < threshold] = 0\n",
    "labels[labels >= threshold] = 1\n",
    "                                                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that encodes the sentences by their part-of-speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences_POS(sentences):\n",
    "    scaler = MinMaxScaler() \n",
    "    dataset = []\n",
    "    for i in range(len(sentences)):\n",
    "        # tokenize the sentence\n",
    "        tokens = nltk.word_tokenize(sentences[i])\n",
    "        # tag the all the tokens\n",
    "        pos_tokens = nltk.pos_tag(tokens)\n",
    "        # for each pos tag, count how many times it occurs in the sentence\n",
    "        pos_dict = generate_POS_dict()\n",
    "        # check if certain keywords are present in the sentence\n",
    "        keyword_dict = generate_keyword_dict()\n",
    "        for tag in pos_tokens:\n",
    "            if tag[1] in pos_dict:\n",
    "                pos_dict[tag[1]] += 1\n",
    "        # get the pos tag counts as features \n",
    "        pos_vector = list(pos_dict.values())\n",
    "        \n",
    "        for key in keyword_dict:\n",
    "            if(key in sentences[i].lower()):\n",
    "                keyword_dict[key] += 1\n",
    "        keyword_vector = list(keyword_dict.values())\n",
    "        \n",
    "        feature_vector = pos_vector \n",
    "        # compute the average word length of the sentence\n",
    "        #n_chars = 0\n",
    "        #avg_len = 0\n",
    "        \"\"\"for token in tokens:\n",
    "            if(token not in \".,?!\"):\n",
    "                n_chars += len(token)\n",
    "        avg_len = n_chars / len(tokens)\"\"\"\n",
    "        \n",
    "        #feature_vector.append(avg_len)\n",
    "        feature_vector.append(len(tokens))\n",
    "        dataset.append(feature_vector)\n",
    "    #dataset = pd.DataFrame(dataset)\n",
    "    #dataset[len(feature_vector) - 1] = dataset[len(feature_vector) - 1] / max(dataset[len(feature_vector) - 1])\n",
    "    #return dataset.values\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_POS_dict():\n",
    "    #tags = ['IN', 'PRP', 'VBP', 'TO', 'VB', 'JJ', 'NN', 'VBZ', 'VBN', 'DT', 'NNP', 'VBD', '.',\n",
    "    #       'CC', 'CD', 'EX', 'FW', 'JJR', 'JJS', 'LS', 'WP', 'WP$', 'WRB']\n",
    "    tags = ['NN', 'IN', 'DT', 'PRP', 'RB', 'JJ', '.', 'VB', 'NNS', ',', 'SYM']    \n",
    "    dic = dict.fromkeys(tags, 0)\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keyword_dict():\n",
    "    words = ['because', 'but', 'since', 'reason', 'that', 'either', 'nonetheless', 'example', 'so']\n",
    "    dic = dict.fromkeys(words, 0)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that encodes the sentence based on the words within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences_BOW(sentences, BOW_dict):\n",
    "    dataset = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence_dict = BOW_dict.copy()\n",
    "        for key in BOW_dict:\n",
    "            if(key in sentences[i]):\n",
    "                sentence_dict[key] = 1\n",
    "        dataset.append(list(sentence_dict.values()))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_BOW_dict(sentences):\n",
    "    # concatenate all argument sentences into one big string\n",
    "    all_sentences = \" \".join(sentences)\n",
    "    # construct a dictionary containing all the words that occur in the sentences\n",
    "    all_tokens = nltk.word_tokenize(all_sentences)\n",
    "    sno = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_tokens = []\n",
    "    for token in all_tokens:\n",
    "        stemmed_tokens.append(sno.stem(token))\n",
    "    feature_dict = dict([token, 0] for token in stemmed_tokens)\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = encode_sentences_POS(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW_dict = generate_BOW_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = encode_sentences_BOW(sentences, BOW_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives: 0.40930232558139534, Positives: 0.5906976744186047\n"
     ]
    }
   ],
   "source": [
    "N = len(dataset)\n",
    "Xtrain = dataset[0:int(0.8*len(dataset))]\n",
    "Ytrain = labels[0:int(0.8*len(dataset))]\n",
    "\n",
    "Xtest = dataset[int(0.8*len(dataset)):]\n",
    "Ytest = labels[int(0.8*len(dataset)):]\n",
    "\n",
    "negatives = Ytest[Ytest < threshold]\n",
    "positives = Ytest[Ytest >= threshold]\n",
    "print(\"Negatives: {0}, Positives: {1}\".format(len(negatives) / len(Xtest), len(positives) / len(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a MLP on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'abs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-88b4d8558793>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mMLP\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#mlp = MLPClassifier()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#lm = LogisticRegression()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AI\\Year2\\blockA\\arguingAgents\\ArguingAgents\\MLP.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Activation functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0melu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhard_sigmoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\activations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_base_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclip_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariable_scope\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariables\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# TODO(fchollet): Remove hourglass imports once external code is done importing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# non-public APIs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'abs'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from MLP import *\n",
    "#mlp = MLPClassifier()\n",
    "#lm = LogisticRegression()\n",
    "#svm =  LinearSVC() \n",
    "#nb = BernoulliNB()\n",
    "#mlp = Network(n_features = len(Xtrain[0]), architecture = [len(Xtrain[0]), 50, 1], n_outputs = 1, learning_rate = 0.01)\n",
    "#sk_mlp = MLPClassifier()\n",
    "\n",
    "\n",
    "# try out regression classifiers\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "sk_mlp = MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lm.fit(Xtrain, Ytrain)\n",
    "#svm.fit(Xtrain, Ytrain)\n",
    "#nb.fit(Xtrain, Ytrain)\n",
    "Ytrain = Ytrain.reshape(len(Ytrain), 1)\n",
    "mlp.train(Xtrain, Ytrain, 5000, 100)\n",
    "#sk_mlp.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well the classifiers score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mlp.score(Xtest, Ytest))\n",
    "#print(lm.score(Xtest, Ytest))\n",
    "#print(svm.score(Xtest, Ytest))\n",
    "#print(nb.score(Xtest, Ytest))\n",
    "Ytest = Ytest.reshape(len(Ytest), 1)\n",
    "print(mlp.accuracy(Xtest, Ytest))\n",
    "#print(sk_mlp.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that splits text into argument sentences and non-argument/bad argument sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_post_binary(text):\n",
    "    sentences = text.split('.')[:-1] \n",
    "    #encoded_sentences = encode_sentences_BOW(sentences, BOW_dict)\n",
    "    encoded_sentences = encode_sentences_POS(sentences)\n",
    "    filtered_text = []\n",
    "    removed_sentences = []\n",
    "    for i in range(len(encoded_sentences)):\n",
    "        #prediction = mlp.predict([encoded_sentences[i]])\n",
    "        prediction = lm.predict([encoded_sentences[i]])\n",
    "        #conf = mlp.predict_proba([encoded_sentences[i]])[0][1]\n",
    "        conf_lm = lm.predict_proba([encoded_sentences[i]])[0][1]  \n",
    "        #conf_svm = svm.predict_proba([encoded_sentences[i]])[0][1] \n",
    "        conf_nb = nb.predict_proba([encoded_sentences[i]])[0][1] \n",
    "        conf_mlp = sk_mlp.predict_proba([encoded_sentences[i]])[0][1] \n",
    "        print(conf_lm, conf_nb, conf_mlp)\n",
    "        conf = np.mean([conf_lm, conf_nb, conf_mlp])\n",
    "        if(prediction == 1 and conf >= 0.8):     \n",
    "            filtered_text.append((conf, sentences[i]))\n",
    "        else:\n",
    "            #conf = mlp.predict_proba([encoded_sentences[i]])[0][1]\n",
    "            removed_sentences.append((conf, sentences[i]))\n",
    "\n",
    "    print(\"{0} filtered from the input text!\".format(len(removed_sentences)))\n",
    "    return filtered_text, removed_sentences\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_post_regression(text):\n",
    "    sentences = text.split('.')[:-1] \n",
    "    #encoded_sentences = encode_sentences_BOW(sentences, BOW_dict)\n",
    "    encoded_sentences = encode_sentences_POS(sentences)\n",
    "    filtered_text = []\n",
    "    removed_sentences = []\n",
    "    for i in range(len(encoded_sentences)):\n",
    "        score = mlp.test([encoded_sentences[i]])\n",
    "        print(score)\n",
    "        if(score >= 0.6):     \n",
    "            filtered_text.append((score, sentences[i]))\n",
    "        else:\n",
    "            removed_sentences.append((score, sentences[i]))\n",
    "\n",
    "    print(\"{0} filtered from the input text!\".format(len(removed_sentences)))\n",
    "    return filtered_text, removed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_post = \"The classic example of this would be a person’s Grandparents that grew up in a different time where it was not considered a bad thing to classify and ultimately look down on a person based on their perceived racial category. Many of not most of the time this person has had limited sustained contact with people of other races and is at times arrogantly used to making broad statements about members of certain racial categories. Most people people don’t have any problems referring their grandparents as racists. Their way of thinking didn’t and doesn’t automatically prevent them from being a good parent, spouse, grandparent, citizen, etc. The difficulty today is that people are unwilling to come to terms with the fact that they have racist beliefs, often inherited from older generations that can subtly (and not so subtly) come out in their interactions with people of different races. Racist to them equal someone that is evil and it feels bad to be labeled that way. Ultimately being a racist is a negative quality that many otherwise good people have. Being racist can and does cause harm to others and people should be proactive in changing their racist beliefs as a method of self improvement. There certainly are evil people who maliciously harbor racial hatred against others that are racists too. We often consider them more representative of the term Racist than your Grandparents or instance but this is just a matter of degree.If a person calls you a racist there is always the possibility that they are right. This should be an opportunity for self reflection rather than an automatic denial.\"\n",
    "\n",
    "text, filtered = filter_post_regression(OP_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = \"Ok so I'm not if I should be changing your view on if racists are evil, or if racists are good people. Personally I don't believe racists are good people, I also don't believe they are bad or evil people either, UNLESS they use their ignorance to either do harm or not do anything. For example someone doesn't like black people and refuses to hire them at their work, or refuses to help a black person who is in need.By that same token it's the same with people who don't like kids for instance. You're not good, bad, or evil, UNLESS you use it to harm or not help. Like people who don't like kids and refuse to help children in need, or hurt children. Racial biases, stereotypes, and prejudices exist in the older generations for sure, and no I wouldn't say it is evil, but I wouldn't classify them as good people either. My grandmother is a racist and I don't classify her as good, because it's ignorant as fuck. I also don't classify her as evil for holding beliefs that she hasn't promoted or done harm with. Doesn't mean I have to like her or associate with her for example.\"\n",
    "text, filtered = filter_post_regression(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_str = \"It's the same as raising children. In order for them to have the best possible outcome in life, you have to sometimes sacrifice their short-term well-being and hurt their feelings in order for them to have a better chance at long-term happiness. It's the same with fat people, in order for them to have a happy and prosperous long-term future, you (you as in society, friends, family or whatever) have to sacrifice their short-term happiness to achieve this. But that is not ok for some reason. While if someone told you 'I just let my kid do what he wants to whoever he wants and I never tell him what and when he is wrong because I want him to be happy', most normal people would think those are some sucky parents. And just so I try to remove some strawman arguments, I don't include people who have some medical or other problems which are causing them to gain weight. I'm talking about most fat people who are just too lazy and have no self-control when it comes to food. I also don't think we should go around and yell at fat people or try to change them the way parents do to their kids. Im talking about overall societal view to these issue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, filtered = filter_post_regression(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = \"Who are these people going around telling fat people that it is ok to be fat? I don't think this is an even remotely common occurrence. In my experience people tend to agree (the 'overall societal view') that it is not ok/good/healthy to be fat, but we shouldn't treat people negatively just because they are fat. Accepting people and treating them with kindness is not the same as telling them there is no need to get in shape. Fat people are reminded constantly by society (especially healthcare professionals) that they should work on losing weight.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, filtered = filter_post_regression(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"I realize this is illegal in some places, but it should be illegal everywhere. Putting flyers in people's car windows or in their windshield wipers is basically littering. The owners of the cars never consented to having pieces of paper be left on their property. If you want to advertise your business using flyers, have people hand them out to people. Don't just leave it on their car. Will that be more expensive paying someone by the hour to hand people flyers? Yes. But that's the business owner's problem. An attempt at cheap advertisement does not justify putting shit on people's cars. 'But think of the small businesses' will not change my view.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, filtered = filter_post_regression(new_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
