{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import Google's trained word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format ('models/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare the data by encoding every sentence as a sequence of word2vec-encoded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dp = pd.read_csv(\"data/dp-slider-means.csv\")\n",
    "topic = pd.Series([\"death penalty\"] * len(raw_data_dp), name = \"topic\")\n",
    "raw_data_dp[\"topic\"] = topic\n",
    "raw_data_evo = pd.read_csv(\"data/evo-slider-means.csv\")\n",
    "topic = pd.Series([\"evolution\"] * len(raw_data_evo), name = \"topic\")\n",
    "raw_data_evo[\"topic\"] = topic\n",
    "raw_data_gc = pd.read_csv(\"data/gc-slider-means.csv\")\n",
    "topic = pd.Series([\"gun control\"] * len(raw_data_gc), name = \"topic\")\n",
    "raw_data_gc[\"topic\"] = topic\n",
    "raw_data_gm = pd.read_csv(\"data/gm-slider-means.csv\")\n",
    "topic = pd.Series([\"gay marriage\"] * len(raw_data_gm), name = \"topic\")\n",
    "raw_data_gm[\"topic\"] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [raw_data_dp, raw_data_evo, raw_data_gc, raw_data_gm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat(frames, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 5375 rows and 9 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The combined dataset contains {0} rows and {1} columns\".format(len(raw_data), len(raw_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to keep three columns: The topic to compute the topic-relevance of a sentence, the sentence itself and the label of the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[[\"topic\", \"Phrase.x\", \"GoodSliderMean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>Phrase.x</th>\n",
       "      <th>GoodSliderMean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>death penalty</td>\n",
       "      <td>Sorry for the length of the post, but I hope i...</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>death penalty</td>\n",
       "      <td>I am all for the death penalty.</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>death penalty</td>\n",
       "      <td>I am pro death penalty.</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>death penalty</td>\n",
       "      <td>I can't believe that you just said \"So what if...</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>death penalty</td>\n",
       "      <td>So what does he have to do with a debate like ...</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           topic                                           Phrase.x  \\\n",
       "0  death penalty  Sorry for the length of the post, but I hope i...   \n",
       "1  death penalty                    I am all for the death penalty.   \n",
       "2  death penalty                            I am pro death penalty.   \n",
       "3  death penalty  I can't believe that you just said \"So what if...   \n",
       "4  death penalty  So what does he have to do with a debate like ...   \n",
       "\n",
       "   GoodSliderMean  \n",
       "0           1.000  \n",
       "1           1.000  \n",
       "2           1.000  \n",
       "3           1.000  \n",
       "4           0.999  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.rename(index=str, columns={\"Phrase.x\": \"sentence\", \"GoodSliderMean\": \"annotation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def encode_sentences(data):\n",
    "    N_sentences = len(data) \n",
    "    encoded_sentences = []\n",
    "    print(\"---------------------Now encoding sentences!---------------------\")\n",
    "    print(\"Max iterations:\", N_sentences)\n",
    "    # shuffle the dataframe rows\n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    labels = data.annotation.copy()\n",
    "    labels = labels.values\n",
    "    \n",
    "    \n",
    "    # take the topic that the sentence comes from,\n",
    "    # to compute topic relevance\n",
    "    topics = data.topic\n",
    "    topics = list(topics)\n",
    "    \n",
    "    # Store the different amount of word counts,\n",
    "    # together with the indices of the sentences\n",
    "    # that contain this amount of words\n",
    "    word_counts = {}\n",
    "    \n",
    "    max_words = 0\n",
    "    \n",
    "    \n",
    "    # for each sentence:\n",
    "    for i in range(N_sentences):\n",
    "        # take the sentence from the dataframe\n",
    "        sentence = data.sentence.iloc[i]\n",
    "        # tokenize the sentence\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        # filter puncuation and stop words from the tokens\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if(token[0] not in \".,:;[](){}!?-_`'~\\\"^/1234567890\"):\n",
    "                words.append(token)\n",
    "        N_words = len(words)\n",
    "        \n",
    "        # keep track of the maximum sentence length\n",
    "        if(N_words > max_words):\n",
    "            max_words = N_words\n",
    "        \n",
    "        \n",
    "        # if this amount of words has been\n",
    "        # encountered before, add the index\n",
    "        # of the sentence\n",
    "        if(N_words in word_counts):\n",
    "            word_counts[N_words].append(i)\n",
    "        # else, create new entry with index\n",
    "        else:\n",
    "             word_counts[N_words] = [i]\n",
    "     \n",
    "        # encode topic and add similarity of sentence to topic\n",
    "        # as additional feature\n",
    "        topic = topics[i]\n",
    "        topic_words = topic.split()\n",
    "        topic_vectors = []\n",
    "        # compute the average word vector for the topic\n",
    "        for word in topic_words:\n",
    "            if(word in word2vec):\n",
    "                word_vector = word2vec[word]\n",
    "            else:\n",
    "                word_vector = np.random.uniform(low = -0.01, high = 0.01, size = (300))\n",
    "            topic_vectors.append(word_vector)\n",
    "        topic_vectors = np.asarray(topic_vectors)\n",
    "        avg_topic_vector = np.mean(topic_vectors, axis = 0)\n",
    "            \n",
    "        # store a sentence as a sequence of word vectors\n",
    "        sequence = []\n",
    "        for word in words:\n",
    "            # embed a word using the Google word2vec model,\n",
    "            # if it exists in the dictionary\n",
    "            if(word in word2vec):\n",
    "                 word_vector = word2vec[word]\n",
    "            # if word does not exist in the word2vec model, \n",
    "            # add a randomized word vector instead\n",
    "            else:\n",
    "                word_vector = np.random.uniform(low = -0.01, high = 0.01, size = (300))\n",
    "        \n",
    "            \n",
    "            # compute similarity between word and topic, then add as feature\n",
    "            similarity = cosine_similarity([word_vector], [avg_topic_vector])\n",
    "            #print(\"Current word:\", word, \"Curren topic:\",  topic,  \"similarity:\", similarity)\n",
    "            word_vector = np.append(word_vector, similarity)     \n",
    "            # add word to the sequence\n",
    "            sequence.append(word_vector)\n",
    "        # convert list sequence to numpy array for convenience\n",
    "        sequence = np.asarray(sequence)\n",
    "        # print progress every 1000 epochs\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"iteration :\", i )\n",
    "        encoded_sentences.append(sequence)\n",
    "        \n",
    "    encoded_sentences = np.asarray(encoded_sentences)\n",
    "    \n",
    "    \n",
    "    \"\"\"print(\"Now zero padding..\")\n",
    "    for i in range(N_sentences):\n",
    "        # print progress every 1000 epochs\n",
    "        if(i % 1000 == 0):\n",
    "            print(\"iteration :\", i )\n",
    "        # compute how much zero padding is needed\n",
    "        N_words = len(encoded_sentences[i])\n",
    "        padding_needed = max_words - N_words\n",
    "        for j in range(padding_needed):\n",
    "            encoded_sentences[i] = np.append(encoded_sentences[i], [None], axis = 0)\"\"\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    # create batches to speed-up training\n",
    "    # group sentences with equal word counts into the same batches\n",
    "    all_batches = []\n",
    "    label_batches = []\n",
    "    #print(max_words)\n",
    "    #print(word_counts)\n",
    "    for count in word_counts:\n",
    "        # get the sentences with this amount of words\n",
    "        sentence_idx = word_counts[count]\n",
    "        batch = []\n",
    "        label_batch = []\n",
    "        # add each sentence with this amount of words\n",
    "        # to the batch\n",
    "        for idx in sentence_idx:\n",
    "            batch.append(encoded_sentences[idx])\n",
    "            label_batch.append(labels[idx])\n",
    "            #print(label_batch)\n",
    "        batch = np.asarray(batch)\n",
    "        label_batch = np.asarray(label_batch)\n",
    "        \n",
    "        all_batches.append(batch)\n",
    "        label_batches.append(label_batch)\n",
    "        \n",
    "    all_batches = np.asarray(all_batches)\n",
    "    label_batches = np.asarray(label_batches)\n",
    "    # now, all the different batches are stored in an\n",
    "    # array, where each batch can be accessed by an \n",
    "    # index\n",
    "    return all_batches, label_batches, data\n",
    "    \n",
    "    \n",
    "    #return encoded_sentences, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the annotation as the labels and convert all the arguments to the positive class and the non-arguments to the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Now encoding sentences!---------------------\n",
      "Max iterations: 5375\n",
      "iteration : 0\n",
      "iteration : 1000\n",
      "iteration : 2000\n",
      "iteration : 3000\n",
      "iteration : 4000\n",
      "iteration : 5000\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences, labels, shuffled_data = encode_sentences(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_sentences, labels = encode_sentences(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_sentences[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encoded_sentences.shape)\n",
    "#encoded_sentences = pad_sequences(encoded_sentences, padding = \"post\", value = np.zeros(301), maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(encoded_sentences)\n",
    "train_test_split = 0.5\n",
    "validation_size = (1 - train_test_split) / 2\n",
    "x_train = encoded_sentences[:int(train_test_split*N)]\n",
    "y_train = labels[:int(train_test_split*N)]\n",
    "\n",
    "x_val = encoded_sentences[int(train_test_split*N) : int(train_test_split*N) + int(validation_size*N)]\n",
    "y_val = labels[int(train_test_split*N) : int(train_test_split*N) + int(validation_size*N)]\n",
    "\n",
    "x_test = encoded_sentences[int(train_test_split*N) + int(validation_size*N):]\n",
    "y_test = labels[int(train_test_split*N) + int(validation_size*N):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the Keras LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "def init_model(size = 32, dropout = 0.7, learning_rate = 0.005):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(size), merge_mode='concat', input_shape=(None, 301)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, labels):\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    for i in range(len(data)):\n",
    "        score = model.evaluate(data[i], labels[i], verbose = 0)\n",
    "        #print(score)\n",
    "        total_accuracy += score[1]\n",
    "        total_loss += score[0]\n",
    "    print(\"Test/validation loss:\", total_loss)\n",
    "    total_accuracy = total_accuracy / len(data)\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold(data, labels, folds, epochs):\n",
    "    kf = KFold(n_splits=folds)\n",
    "\n",
    "    avg_accuracy = 0\n",
    "    current_fold = 0\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # reset the classifier\n",
    "        model = init_model()\n",
    "        \n",
    "        \n",
    "        Xtrain = data[train_index]\n",
    "        Xtest = data[test_index]\n",
    "        \n",
    "        Ytrain = labels[train_index]\n",
    "        Ytest = labels[test_index]\n",
    "        \n",
    "        # train on the train data\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(Xtrain)):\n",
    "                model.fit(Xtrain[i], Ytrain[i], epochs=1, verbose = 0, batch_size = Xtrain[i].shape[0])\n",
    "            # get train accuracy\n",
    "            train_acc = get_accuracy(Xtrain, Ytrain)\n",
    "            print(\"Fold: {0} \\n Epoch: {1} \\n Train accuracy: {2}\".format(current_fold, e, train_acc))\n",
    "                \n",
    "        # test on the test data\n",
    "        test_acc = get_accuracy(Xtest, Ytest)\n",
    "        print(\"Fold: {0} \\n Test accuracy: {2}\".format(current_fold, test_acc))\n",
    "        avg_accuracy += test_acc \n",
    "        current_fold += 1\n",
    "            \n",
    "    avg_accuracy /= folds\n",
    "    print(\"{0}-fold cross validation accuracy: {1}\".format(folds, avg_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 \n",
      " Epoch: 0 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 1 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 2 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 3 \n",
      " Train accuracy: 0.5035470218752873\n",
      "Fold: 0 \n",
      " Epoch: 4 \n",
      " Train accuracy: 0.5035470218752873\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ef8887babf5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-6f85e9c35fdf>\u001b[0m in \u001b[0;36mkfold\u001b[1;34m(data, labels, folds, epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m# get train accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold(encoded_sentences, labels, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of batches: 46\n",
      "--------------Training epoch:-------------- 0\n",
      "Loss:  12.63010449707508\n",
      "Test/validation loss: 3.7287307754158974\n",
      "--------------Training epoch:-------------- 1\n",
      "Loss:  10.269999161362648\n",
      "Test/validation loss: 5.73493005335331\n",
      "--------------Training epoch:-------------- 2\n",
      "Loss:  9.360667459666729\n",
      "Test/validation loss: 4.025868728756905\n",
      "--------------Training epoch:-------------- 3\n",
      "Loss:  8.96983977407217\n",
      "Test/validation loss: 3.4201946184039116\n",
      "--------------Training epoch:-------------- 4\n",
      "Loss:  8.301468674093485\n",
      "Test/validation loss: 3.3801503106951714\n",
      "--------------Training epoch:-------------- 5\n",
      "Loss:  8.194673113524914\n",
      "Test/validation loss: 3.411381222307682\n",
      "--------------Training epoch:-------------- 6\n",
      "Loss:  7.35138463228941\n",
      "Test/validation loss: 3.629140518605709\n",
      "--------------Training epoch:-------------- 7\n",
      "Loss:  7.179097309708595\n",
      "Test/validation loss: 3.2782297767698765\n",
      "--------------Training epoch:-------------- 8\n",
      "Loss:  7.05914580821991\n",
      "Test/validation loss: 3.8716991096735\n",
      "--------------Training epoch:-------------- 9\n",
      "Loss:  6.8643209878355265\n",
      "Test/validation loss: 3.3826174587011337\n",
      "--------------Training epoch:-------------- 10\n",
      "Loss:  6.6542205065488815\n",
      "Test/validation loss: 3.5201938189566135\n",
      "--------------Training epoch:-------------- 11\n",
      "Loss:  6.445310201495886\n",
      "Test/validation loss: 3.523382965475321\n",
      "--------------Training epoch:-------------- 12\n",
      "Loss:  6.36584085971117\n",
      "Test/validation loss: 3.4383584782481194\n",
      "--------------Training epoch:-------------- 13\n",
      "Loss:  6.197158440947533\n",
      "Test/validation loss: 3.45573940128088\n",
      "--------------Training epoch:-------------- 14\n",
      "Loss:  6.268889088183641\n",
      "Test/validation loss: 3.8257273361086845\n",
      "--------------Training epoch:-------------- 15\n",
      "Loss:  6.030932743102312\n",
      "Test/validation loss: 3.3948664478957653\n",
      "--------------Training epoch:-------------- 16\n",
      "Loss:  5.970652125775814\n",
      "Test/validation loss: 3.459564547985792\n",
      "--------------Training epoch:-------------- 17\n",
      "Loss:  5.803834326565266\n",
      "Test/validation loss: 3.300027597695589\n",
      "--------------Training epoch:-------------- 18\n",
      "Loss:  5.81340317428112\n",
      "Test/validation loss: 3.3635864853858948\n",
      "--------------Training epoch:-------------- 19\n",
      "Loss:  5.7719441913068295\n",
      "Test/validation loss: 3.849947050213814\n",
      "--------------Training epoch:-------------- 20\n",
      "Loss:  5.607327256351709\n",
      "Test/validation loss: 3.4111140109598637\n",
      "--------------Training epoch:-------------- 21\n",
      "Loss:  5.471315387636423\n",
      "Test/validation loss: 3.169040732085705\n",
      "--------------Training epoch:-------------- 22\n",
      "Loss:  5.460796672850847\n",
      "Test/validation loss: 3.5883443877100945\n",
      "--------------Training epoch:-------------- 23\n",
      "Loss:  5.369416061788797\n",
      "Test/validation loss: 3.965450882911682\n",
      "--------------Training epoch:-------------- 24\n",
      "Loss:  5.3498940616846085\n",
      "Test/validation loss: 3.5696949772536755\n",
      "--------------Training epoch:-------------- 25\n",
      "Loss:  5.381875492632389\n",
      "Test/validation loss: 3.7180737741291523\n",
      "--------------Training epoch:-------------- 26\n",
      "Loss:  5.120507959276438\n",
      "Test/validation loss: 3.5491353161633015\n",
      "--------------Training epoch:-------------- 27\n",
      "Loss:  4.977482553571463\n",
      "Test/validation loss: 3.6994083300232887\n",
      "--------------Training epoch:-------------- 28\n",
      "Loss:  5.153417222201824\n",
      "Test/validation loss: 3.5005050115287304\n",
      "--------------Training epoch:-------------- 29\n",
      "Loss:  4.809761464595795\n",
      "Test/validation loss: 4.232271701097488\n",
      "--------------Training epoch:-------------- 30\n",
      "Loss:  5.090752515941858\n",
      "Test/validation loss: 3.655243758112192\n",
      "--------------Training epoch:-------------- 31\n",
      "Loss:  5.007652599364519\n",
      "Test/validation loss: 3.854767546057701\n",
      "--------------Training epoch:-------------- 32\n",
      "Loss:  4.77236108481884\n",
      "Test/validation loss: 3.4288357086479664\n",
      "--------------Training epoch:-------------- 33\n",
      "Loss:  4.749987635761499\n",
      "Test/validation loss: 3.710905507206917\n",
      "--------------Training epoch:-------------- 34\n",
      "Loss:  4.83488405495882\n",
      "Test/validation loss: 3.553715229034424\n",
      "--------------Training epoch:-------------- 35\n",
      "Loss:  4.684839421883225\n",
      "Test/validation loss: 3.76697626337409\n",
      "--------------Training epoch:-------------- 36\n",
      "Loss:  4.678625259548426\n",
      "Test/validation loss: 3.4483039379119873\n",
      "--------------Training epoch:-------------- 37\n",
      "Loss:  4.608506880700588\n",
      "Test/validation loss: 3.6210257075726986\n",
      "--------------Training epoch:-------------- 38\n",
      "Loss:  4.438675969839096\n",
      "Test/validation loss: 3.452754184603691\n",
      "--------------Training epoch:-------------- 39\n",
      "Loss:  4.475540857762098\n",
      "Test/validation loss: 3.8919886015355587\n",
      "--------------Training epoch:-------------- 40\n",
      "Loss:  4.414233114570379\n",
      "Test/validation loss: 3.559327144175768\n",
      "--------------Training epoch:-------------- 41\n",
      "Loss:  4.650852048769593\n",
      "Test/validation loss: 3.6317502073943615\n",
      "--------------Training epoch:-------------- 42\n",
      "Loss:  4.41186997666955\n",
      "Test/validation loss: 3.539007041603327\n",
      "--------------Training epoch:-------------- 43\n",
      "Loss:  4.385693896561861\n",
      "Test/validation loss: 3.5763590671122074\n",
      "--------------Training epoch:-------------- 44\n",
      "Loss:  4.290796983987093\n",
      "Test/validation loss: 3.8317734226584435\n",
      "--------------Training epoch:-------------- 45\n",
      "Loss:  4.436337981373072\n",
      "Test/validation loss: 3.9450428374111652\n",
      "--------------Training epoch:-------------- 46\n",
      "Loss:  4.216905176639557\n",
      "Test/validation loss: 3.83045469596982\n",
      "--------------Training epoch:-------------- 47\n",
      "Loss:  4.294996850192547\n",
      "Test/validation loss: 4.162487596273422\n",
      "--------------Training epoch:-------------- 48\n",
      "Loss:  4.2636132799088955\n",
      "Test/validation loss: 3.6849340572953224\n",
      "--------------Training epoch:-------------- 49\n",
      "Loss:  4.219406899064779\n",
      "Test/validation loss: 3.7708572037518024\n",
      "--------------Training epoch:-------------- 50\n",
      "Loss:  4.146628454327583\n",
      "Test/validation loss: 3.7267504557967186\n",
      "--------------Training epoch:-------------- 51\n",
      "Loss:  4.172445047646761\n",
      "Test/validation loss: 3.7359180375933647\n",
      "--------------Training epoch:-------------- 52\n",
      "Loss:  4.005782727152109\n",
      "Test/validation loss: 4.127639699727297\n",
      "--------------Training epoch:-------------- 53\n",
      "Loss:  4.1019603088498116\n",
      "Test/validation loss: 3.9950474239885807\n",
      "--------------Training epoch:-------------- 54\n",
      "Loss:  4.144301403313875\n",
      "Test/validation loss: 3.854729328304529\n",
      "--------------Training epoch:-------------- 55\n",
      "Loss:  4.121590327471495\n",
      "Test/validation loss: 3.894834689795971\n",
      "--------------Training epoch:-------------- 56\n",
      "Loss:  4.139665387570858\n",
      "Test/validation loss: 3.7939771339297295\n",
      "--------------Training epoch:-------------- 57\n",
      "Loss:  4.020369865000248\n",
      "Test/validation loss: 3.98761086165905\n",
      "--------------Training epoch:-------------- 58\n",
      "Loss:  3.984352856874466\n",
      "Test/validation loss: 4.196136623620987\n",
      "--------------Training epoch:-------------- 59\n",
      "Loss:  4.138663604855537\n",
      "Test/validation loss: 3.980134427547455\n",
      "--------------Training epoch:-------------- 60\n",
      "Loss:  3.9621448814868927\n",
      "Test/validation loss: 3.7399085387587547\n",
      "--------------Training epoch:-------------- 61\n",
      "Loss:  3.8636814299970865\n",
      "Test/validation loss: 3.899660386145115\n",
      "--------------Training epoch:-------------- 62\n",
      "Loss:  3.828877091407776\n",
      "Test/validation loss: 3.9458456225693226\n",
      "--------------Training epoch:-------------- 63\n",
      "Loss:  3.8114191498607397\n",
      "Test/validation loss: 4.092084560543299\n",
      "--------------Training epoch:-------------- 64\n",
      "Loss:  3.874022740870714\n",
      "Test/validation loss: 4.208241108804941\n",
      "--------------Training epoch:-------------- 65\n",
      "Loss:  3.757600776851177\n",
      "Test/validation loss: 4.287384957075119\n",
      "--------------Training epoch:-------------- 66\n",
      "Loss:  3.9254600927233696\n",
      "Test/validation loss: 4.174856122583151\n",
      "--------------Training epoch:-------------- 67\n",
      "Loss:  3.8333514537662268\n",
      "Test/validation loss: 4.474733367562294\n",
      "--------------Training epoch:-------------- 68\n",
      "Loss:  3.728559210896492\n",
      "Test/validation loss: 4.134955286979675\n",
      "--------------Training epoch:-------------- 69\n",
      "Loss:  3.63542952015996\n",
      "Test/validation loss: 4.338808745145798\n",
      "--------------Training epoch:-------------- 70\n",
      "Loss:  3.723917480558157\n",
      "Test/validation loss: 4.053731709718704\n",
      "--------------Training epoch:-------------- 71\n",
      "Loss:  3.703106727451086\n",
      "Test/validation loss: 4.090519715100527\n",
      "--------------Training epoch:-------------- 72\n",
      "Loss:  3.705524280667305\n",
      "Test/validation loss: 5.250475861132145\n",
      "--------------Training epoch:-------------- 73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.648715201765299\n",
      "Test/validation loss: 4.1165606044232845\n",
      "--------------Training epoch:-------------- 74\n",
      "Loss:  3.5173948854207993\n",
      "Test/validation loss: 4.001003663986921\n",
      "--------------Training epoch:-------------- 75\n",
      "Loss:  3.602947753854096\n",
      "Test/validation loss: 4.093642067164183\n",
      "--------------Training epoch:-------------- 76\n",
      "Loss:  3.5028557889163494\n",
      "Test/validation loss: 3.7575416043400764\n",
      "--------------Training epoch:-------------- 77\n",
      "Loss:  3.5886630825698376\n",
      "Test/validation loss: 4.010238245129585\n",
      "--------------Training epoch:-------------- 78\n",
      "Loss:  3.6469405479729176\n",
      "Test/validation loss: 3.9628771245479584\n",
      "--------------Training epoch:-------------- 79\n",
      "Loss:  3.5191378043964505\n",
      "Test/validation loss: 3.935254964977503\n",
      "--------------Training epoch:-------------- 80\n",
      "Loss:  3.592995159327984\n",
      "Test/validation loss: 3.929815847426653\n",
      "--------------Training epoch:-------------- 81\n",
      "Loss:  3.4314150474965572\n",
      "Test/validation loss: 3.7188501581549644\n",
      "--------------Training epoch:-------------- 82\n",
      "Loss:  3.536819703876972\n",
      "Test/validation loss: 3.957193449139595\n",
      "--------------Training epoch:-------------- 83\n",
      "Loss:  3.4904021583497524\n",
      "Test/validation loss: 4.019558530300856\n",
      "--------------Training epoch:-------------- 84\n",
      "Loss:  3.5312965996563435\n",
      "Test/validation loss: 4.189069893211126\n",
      "--------------Training epoch:-------------- 85\n",
      "Loss:  3.372187841683626\n",
      "Test/validation loss: 4.171307111158967\n",
      "--------------Training epoch:-------------- 86\n",
      "Loss:  3.3027554359287024\n",
      "Test/validation loss: 4.4921680316329\n",
      "--------------Training epoch:-------------- 87\n",
      "Loss:  3.3270429745316505\n",
      "Test/validation loss: 4.274221353232861\n",
      "--------------Training epoch:-------------- 88\n",
      "Loss:  3.406846547499299\n",
      "Test/validation loss: 3.9706876687705517\n",
      "--------------Training epoch:-------------- 89\n",
      "Loss:  3.280126381665468\n",
      "Test/validation loss: 4.389357015490532\n",
      "--------------Training epoch:-------------- 90\n",
      "Loss:  3.2278948947787285\n",
      "Test/validation loss: 4.1581931710243225\n",
      "--------------Training epoch:-------------- 91\n",
      "Loss:  3.2974384650588036\n",
      "Test/validation loss: 3.929244499653578\n",
      "--------------Training epoch:-------------- 92\n",
      "Loss:  3.487215965986252\n",
      "Test/validation loss: 3.8632923401892185\n",
      "--------------Training epoch:-------------- 93\n",
      "Loss:  3.4301650188863277\n",
      "Test/validation loss: 4.022516582161188\n",
      "--------------Training epoch:-------------- 94\n",
      "Loss:  3.206856768578291\n",
      "Test/validation loss: 4.104058608412743\n",
      "--------------Training epoch:-------------- 95\n",
      "Loss:  3.2305697053670883\n",
      "Test/validation loss: 3.854838777333498\n",
      "--------------Training epoch:-------------- 96\n",
      "Loss:  3.380519639700651\n",
      "Test/validation loss: 3.968277059495449\n",
      "--------------Training epoch:-------------- 97\n",
      "Loss:  3.25740022957325\n",
      "Test/validation loss: 4.081097222864628\n",
      "--------------Training epoch:-------------- 98\n",
      "Loss:  3.2806839048862457\n",
      "Test/validation loss: 4.016519721597433\n",
      "--------------Training epoch:-------------- 99\n",
      "Loss:  3.3246207796037197\n",
      "Test/validation loss: 4.0996912978589535\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of batches:\", len(x_train))\n",
    "for e in range(epochs):\n",
    "    print(\"--------------Training epoch:--------------\", e)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # shuffle training data and labels\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(x_train)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(y_train)\n",
    "    np.random.seed()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        correct = 0\n",
    "        total += len(x_train[i])\n",
    "        history = model.fit(x_train[i], y_train[i], epochs=1, verbose = 0, batch_size = x_train[i].shape[0])\n",
    "        total_loss += history.history[\"loss\"][0]\n",
    "        acc = history.history['acc'][0]\n",
    "        #print(\"Batch-accuracy:\", acc, \"Samples:\", len(encoded_sentences[i]))\n",
    "\n",
    "        correct = acc * len(x_train[i])\n",
    "        total_correct += correct\n",
    "    #total_loss /= len(x_train)\n",
    "    print(\"Loss: \", total_loss)\n",
    "    #print(\"Train accuracy:\", total_correct / total)\n",
    "    get_accuracy(x_val, y_val)\n",
    "    #print(\"Test accuracy:\", get_accuracy(x_test, y_test))\n",
    "    \n",
    "    #print(\"Accuracy:\", get_accuracy(encoded_sentences, labels))   \n",
    "        \n",
    "\n",
    "    #acc = get_accuracy(x_train, y_train)   \n",
    "    #print(\"Train accuracy:\", acc)\n",
    "    #acc = get_accuracy(x_test, y_test)   \n",
    "    #print(\"Test accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test/validation loss: 4.380858212709427\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\", get_accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/argument_quality.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
